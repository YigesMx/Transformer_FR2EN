{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# proxy\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test google\n",
    "import requests\n",
    "response = requests.get(\"https://www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b919bc6f4395447a8d860f11056c1ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/30.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046cd5c818204aa08d9fefd008e98f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7f23958f9c40ef98c61995d3180db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f5cfe0419b49d4ae2964e15863a52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/232825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464547230a8b443287c2fa8c92f27cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e49f76fde94372ad6d9c4b185be393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = datasets.load_dataset(\"iwslt2017\", \"iwslt2017-en-fr\", split='train')\n",
    "val_dataset = datasets.load_dataset(\"iwslt2017\", \"iwslt2017-en-fr\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': \"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\",\n",
       "  'fr': \"Merci beaucoup, Chris. C'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. Je suis très reconnaissant.\"}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 9379.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 遍历 dataset，获取最长句子 tokenization 后的最长长度\n",
    "from tqdm import tqdm\n",
    "max_length = 0\n",
    "# for example in tqdm(train_dataset):\n",
    "#     tokens = tokenizer.encode(example['translation']['fr'])\n",
    "#     max_length = max(max_length, len(tokens))\n",
    "for example in tqdm(val_dataset):\n",
    "    tokens = tokenizer.encode(example['translation']['fr'])\n",
    "    max_length = max(max_length, len(tokens))\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 10772.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 遍历 dataset，获取最长句子 tokenization 后的最长长度\n",
    "from tqdm import tqdm\n",
    "max_length = 0\n",
    "# for example in tqdm(train_dataset):\n",
    "#     tokens = tokenizer.encode(example['translation']['en'])\n",
    "#     max_length = max(max_length, len(tokens))\n",
    "for example in tqdm(val_dataset):\n",
    "    tokens = tokenizer.encode(example['translation']['en'])\n",
    "    max_length = max(max_length, len(tokens))\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa3ecad6f244efc92d64ddb0f3b290f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dd56ce6f32464db09b949fc13c665c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data.dataset import get_dataloader\n",
    "train_loader = get_dataloader(\"train\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "fr_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "en_fr_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12353, 0], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = en_fr_tokenizer(\"123\")\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12353, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = en_fr_tokenizer.encode(\"123\")\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bien que le requérant demande une indemnisation en dollars des États-Unis, les dispositions financières des accords de crédit-bail étaient libellées en dinars koweïtiens et Santa Fe a calculé son manque à gagner dans cette dernière monnaie.\n",
      "Although the claimant seeks compensation in United States dollars, the payment provisions of the lease agreements were stated in Kuwaiti dinars, and Santa Fe has calculated its claim for loss of revenue in that currency.\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59514\n",
      "0\n",
      "59513\n",
      "59514\n"
     ]
    }
   ],
   "source": [
    "print(fr_en_tokenizer.bos_token_id)\n",
    "print(fr_en_tokenizer.eos_token_id)\n",
    "print(fr_en_tokenizer.pad_token_id)\n",
    "fr_en_tokenizer.add_special_tokens({'bos_token': '<bos>'})\n",
    "print(fr_en_tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1533,    29,    19,  8483,   434,    38, 14174,    23,   678,    13,\n",
      "           206,    21,  1014,     2,    16,   710,  2164,    13,  2617,     5,\n",
      "          2692,    21,  3227,   107,   736, 11332,   114,    23,    20,   190,\n",
      "         10141, 32206,     9,    11,  6979,  8932,    15, 14338,   113,  3104,\n",
      "            17,  6893,    31,   120,  1596,  9001,     3,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded = fr_en_tokenizer(x[0], return_tensors='pt', padding=True, truncation=True, max_length=512) \n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59514\n",
      "0\n",
      "59513\n",
      "59514\n"
     ]
    }
   ],
   "source": [
    "print(en_fr_tokenizer.bos_token_id)\n",
    "print(en_fr_tokenizer.eos_token_id)\n",
    "print(en_fr_tokenizer.pad_token_id)\n",
    "en_fr_tokenizer.add_special_tokens({'bos_token': '<bos>'})\n",
    "print(en_fr_tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 3237,     4, 13663, 11512,  3401,    18,   185,   194,   678,     2,\n",
      "             4,  2688,   968,     7,     4, 14859,  2224,   128,  2645,    18,\n",
      "         31183,  2111, 12766,     9,     2,    10,  6979,  8932,    94,  7193,\n",
      "            96,  3580,    26,  3070,     7,  7427,    18,    33,  7441,     3]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded = en_fr_tokenizer(y[0], return_tensors='pt', padding=True, truncation=True, max_length=512, add_special_tokens=False)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although the claimant seeks compensation in United States dollars, the payment provisions of the lease agreements were stated in Kuwaiti dinars, and Santa Fe has calculated its claim for loss of revenue in that currency.\n"
     ]
    }
   ],
   "source": [
    "print(en_fr_tokenizer.decode(encoded['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[59514,  3237,     4, 13663, 11512,  3401,    18,   185,   194,   678,\n",
      "             2,     4,  2688,   968,     7,     4, 14859,  2224,   128,  2645,\n",
      "            18, 31183,  2111, 12766,     9,     2,    10,  6979,  8932,    94,\n",
      "          7193,    96,  3580,    26,  3070,     7,  7427,    18,    33,  7441,\n",
      "             3,     0, 59513]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded = en_fr_tokenizer(\"<bos>\"+y[0]+\"</s>\"+\"<pad>\", return_tensors='pt', padding=True, truncation=True, max_length=512, add_special_tokens=False)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(en_fr_tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import get_dataloader\n",
    "from tokenizer.fr_en_tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(\"train\", 4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reprise de la session', 'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.', 'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.', 'Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.')\n",
      "('Resumption of the session', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", 'You have requested a debate on this subject in the course of the next few days, during this part-session.')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "x, y = batch\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_tokenizer = Tokenizer(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de1ab5bb94a4df2b466936106a9b8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cc0f257ceb49209553ee421603dded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader2 = get_dataloader(\"train\", 4, shuffle=False, warped_tokenizer=warped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch2 = next(iter(train_loader2))\n",
    "x, y1, y2 = batch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[59514,   660, 14717,  ...,     0,     0,     0],\n",
       "         [59514,   131,  6629,  ...,     0,     0,     0],\n",
       "         [59514,  1247,    88,  ...,     0,     0,     0],\n",
       "         [59514,   344,   867,  ...,     0,     0,     0]]),\n",
       " tensor([[59514,   660, 10252,  ...,     0,     0,     0],\n",
       "         [59514,    47, 15845,  ...,     0,     0,     0],\n",
       "         [59514,  3237,     2,  ...,     0,     0,     0],\n",
       "         [59514,   213,    79,  ...,     0,     0,     0]]),\n",
       " tensor([[  660, 10252,   529,  ...,     0,     0,     0],\n",
       "         [   47, 15845, 15298,  ...,     0,     0,     0],\n",
       "         [ 3237,     2,    48,  ...,     0,     0,     0],\n",
       "         [  213,    79,  2000,  ...,     0,     0,     0]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = lambda x ,y, z: {\"input_ids\": x, \"labels\": y, \"decoder_input_ids\": z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 1.0000, 0.8000],\n",
      "         [1.0000, 1.0000, 0.7071]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义一个余弦相似度损失类\n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "        self.cosine_sim = nn.CosineSimilarity(dim=1)  # 计算通道维上的余弦相似度\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # 计算余弦相似度\n",
    "        cosine_similarity = self.cosine_sim(input1, input2)\n",
    "        # 余弦相似度损失为 1 减去平均余弦相似度\n",
    "        loss = cosine_similarity\n",
    "        return loss\n",
    "\n",
    "# 假设 feature_map1 和 feature_map2 是两个形状为 [batch_size, channels, height, width] 的特征图\n",
    "feature_map1 = torch.tensor([[[[1, 1, 3], [1, 3, 0]], [[1, 1, 4], [1, 3, 1]]]], dtype=float)  # 特征图1\n",
    "feature_map2 = torch.tensor([[[[2, 1, 0], [2, 6, 1]], [[2, 1, 1], [2, 6, 1]]]], dtype=float)  # 特征图2\n",
    "\n",
    "# 创建余弦相似度损失实例\n",
    "cosine_loss = CosineSimilarityLoss()\n",
    "\n",
    "# 计算两个特征图之间的余弦相似度损失\n",
    "loss = cosine_loss(feature_map1, feature_map2)\n",
    "# print(\"Cosine Similarity Loss:\", loss.item())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
